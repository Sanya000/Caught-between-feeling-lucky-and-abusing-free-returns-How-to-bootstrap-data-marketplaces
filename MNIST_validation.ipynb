{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d507c5-bdef-4e69-a22d-45b9ae0bec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "from PIL import Image\n",
    "import itertools\n",
    "from fedlab.utils.dataset.partition import MNISTPartitioner\n",
    "from buyer-insight_algorithm import*\n",
    "from algorithm_greedy import*\n",
    "from algorithm_exploration import*\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203521c-2341-4247-9b83-63a2f5b50a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlippedLabelDataset(Dataset):\n",
    "    def __init__(self, dataset, flip_fraction=0.75):\n",
    "        self.dataset = dataset\n",
    "        self.flip_fraction = flip_fraction\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, label = self.dataset[index]\n",
    "        # Flip the label with a probability of flip_fraction\n",
    "        if np.random.rand() < self.flip_fraction:\n",
    "            # Example of flipping: label -> 9 - label\n",
    "            label = 9 - label\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3dafc-6110-4e0a-85bc-062bdd0e70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_dataloaders(data_loaders, flip_fraction=0.75, indexes):\n",
    "    new_data_loaders = {}\n",
    "    i = 1\n",
    "    for partition_name, loader in data_loaders.items():\n",
    "        if i in indexes:\n",
    "            flipped_dataset = FlippedLabelDataset(loader.dataset, flip_fraction)\n",
    "            new_loader = DataLoader(flipped_dataset, batch_size=loader.batch_size, shuffle=True, num_workers=loader.num_workers)\n",
    "            new_data_loaders[partition_name] = new_loader\n",
    "        else:\n",
    "            new_data_loaders[partition_name] = loader\n",
    "        i += 1\n",
    "    return new_data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a82a52-9822-4cfe-8384-ac17c73e0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, dataset1, dataset2):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.length1 = len(dataset1)\n",
    "        self.targets = dataset1.targets + dataset2.targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset1) + len(self.dataset2)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.length1:\n",
    "            return self.dataset1[idx]\n",
    "        else:\n",
    "            return self.dataset2[idx - self.length1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ecf01e-2f6f-4010-9471-beb192c028b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_loaders(data_loaders, catalogue, original_lengths):\n",
    "    original_keys = list(data_loaders.keys())\n",
    "    combined_loaders = {}\n",
    "    combined_lengths = []  # List to store the number of data points in each combined DataLoader\n",
    "\n",
    "    # Start by adding the original loaders and their lengths\n",
    "    for key, length in zip(original_keys, original_lengths):\n",
    "        combined_loaders[key] = data_loaders[key]\n",
    "        combined_lengths.append(length)  # Add the length of the original DataLoader\n",
    "\n",
    "    # Generate combinations of increasing size\n",
    "    i = len(original_keys) + 1  # Start naming combined loaders from here\n",
    "    for n in range(2, len(original_keys) + 1):\n",
    "        for combo in itertools.combinations(original_keys, n):\n",
    "            if len(combined_loaders) >= catalogue:\n",
    "                return combined_loaders, combined_lengths  # Return early if catalogue limit reached\n",
    "            # Sum the lengths of the datasets in the current combination\n",
    "            combo_length = sum(original_lengths[original_keys.index(k)] for k in combo)\n",
    "            combined_datasets = ConcatDataset([data_loaders[k].dataset for k in combo])\n",
    "            # Assuming all data loaders use the same batch size\n",
    "            batch_size = data_loaders[original_keys[0]].batch_size\n",
    "            combined_loaders[i-1] = DataLoader(combined_datasets, batch_size=batch_size, shuffle=True)\n",
    "            combined_lengths.append(combo_length)  # Append the combined length to the list\n",
    "            i += 1\n",
    "\n",
    "    return combined_loaders, combined_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880601a8-31e5-44c8-b951-3e1ba8f0393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2)  # 16 filters\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 2x2 max pooling\n",
    "        self.fc1 = nn.Linear(16 * 14 * 14, 10)  # Fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Convolution -> ReLU -> Pooling\n",
    "        x = x.view(-1, 16 * 14 * 14)  # Flatten the tensor for the fully connected layer\n",
    "        x = self.fc1(x)  # Fully connected layer\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0800f-4501-48b8-bfde-8373b226c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_quality(loader, batch_size, model):\n",
    "    reset_all_weights(model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, targets) in enumerate(loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            #if (batch_idx + 1) % 100 == 0:\n",
    "                #print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(loader)}], Loss: {loss.item():.4f}\")\n",
    "            \n",
    "        #train_accuracy = calculate_accuracy(model, loader)\n",
    "        #print(f\"Epoch [{epoch + 1}/{num_epochs}] - Training Accuracy: {train_accuracy:.2f}%\")\n",
    "        test_accuracy = calculate_accuracy(model, test_loader)\n",
    "    \n",
    "    #print(\"Training and testing completed @--> \",test_accuracy)\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b89e4-ff5e-4667-9879-c4d4f83614ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    return (correct / total) * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11125176-cc55-450e-88dc-9d3c4159eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_loader(loader):\n",
    "    num_batches = len(loader)\n",
    "    batch_size = loader.batch_size\n",
    "    total_images = num_batches * batch_size\n",
    "\n",
    "    for _, last_batch in enumerate(loader, 1):\n",
    "        pass\n",
    "\n",
    "    last_batch_size = len(last_batch[0])\n",
    "\n",
    "    if last_batch_size < batch_size:\n",
    "        total_images = total_images - batch_size +last_batch_size\n",
    "\n",
    "    return total_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02f5ce-ef6e-458e-9a6c-3ce68120d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 125\n",
    "size_of_V = 10 \n",
    "seed = 2024\n",
    "indexes = [] #List the indexes of the datasets to degrade by label-flipping\n",
    "\n",
    "    \n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=\"/data/MNIST/\", train=True, download=True, transform=transform)\n",
    "    \n",
    "testset = torchvision.datasets.MNIST(root=\"/data/MNIST/\", train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "hetero_dir_part = MNISTPartitioner(trainset.targets, size_of_V, balance=None, partition=\"dirichlet\", dir_alpha=0.3, verbose=False, seed=seed)\n",
    "\n",
    "images_per_seller = np.zeros(size_of_V)\n",
    "for i in range(size_of_V):\n",
    "    images_per_seller[i] = len(hetero_dir_part[i])\n",
    "\n",
    "with open(f\"MNIST_len.json\", 'w') as f:\n",
    "    json.dump(images_per_seller.tolist(), f)\n",
    "    \n",
    "avg_images = np.mean(images_per_seller)\n",
    "    \n",
    "data_loaders = {}\n",
    "partition_indices = hetero_dir_part.client_dict\n",
    "    \n",
    "for partition_name, indices in partition_indices.items():\n",
    "    partition = Subset(trainset, indices)\n",
    "    data_loader = DataLoader(partition, batch_size=batch_size, shuffle = True)\n",
    "    data_loaders[partition_name] = data_loader\n",
    "      \n",
    "\n",
    "if indexes:\n",
    "    complete_data_loaders = modify_dataloaders(data_loaders, 0.75, indexes)\n",
    "else:\n",
    "    complete_data_loaders = data_loaders\n",
    "    \n",
    "catalogue = (2**size_of_V)-1  # Desired number of datasets\n",
    "new_data_loaders, lens = create_combined_loaders(complete_data_loaders, catalogue, images_per_seller)     \n",
    "results = np.zeros((2, catalogue))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for loader_name, train_loader in new_data_loaders.items():\n",
    "    print(f\"Training on loader: {i}\")\n",
    "    model = CNN().to(device)\n",
    "    start_time = time.time()\n",
    "    results[0,i] = fetch_quality(train_loader, batch_size, model)\n",
    "    results[1,i] = time.time() - start_time\n",
    "    print(f\"Accuracy with {i}: {results[0,i]}%, took: {results[1,i]}s\")\n",
    "    with open(f\"MNIST_FM.json\", 'w') as f:\n",
    "        json.dump(results.tolist(), f)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca5d7c-ed24-42e9-9355-f20bc112dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of keys for the combinations in the catalog\n",
    "def generate_keys(catalogue_size, size_of_V):\n",
    "    # Generate the original keys (assuming they are simply integers starting from 1 for simplicity)\n",
    "    original_keys = list(range(1, size_of_V + 1))\n",
    "\n",
    "   # Initialize the list to store the keys representing combinations of original datasets\n",
    "    combination_keys_as_tuples = []\n",
    "\n",
    "    # Start by adding the original datasets' keys as single-element tuples\n",
    "    for key in original_keys:\n",
    "        combination_keys_as_tuples.append((key,))\n",
    "\n",
    "    # Generate combinations of increasing size and create keys representing these combinations as tuples\n",
    "    for n in range(2, len(original_keys) + 1):\n",
    "        for combo in itertools.combinations(original_keys, n):\n",
    "            if len(combination_keys_as_tuples) >= catalogue_size:\n",
    "                # Break early if catalogue limit is reached or about to be reached\n",
    "                break\n",
    "            # Create a tuple from the combo, which already contains integers\n",
    "            combo_tuple = tuple(combo)\n",
    "            combination_keys_as_tuples.append(combo_tuple)\n",
    "\n",
    "    # Check if we reached the limit or need to stop\n",
    "    if len(combination_keys_as_tuples) > catalogue_size:\n",
    "        # Trim the list to the catalogue size if it exceeded the limit\n",
    "        combination_keys_as_tuples = combination_keys_as_tuples[:catalogue_size]\n",
    "\n",
    "    single_key_combinations = [t for t in combination_keys_as_tuples if len(t) == 1]\n",
    "    multiple_key_combinations = [t for t in combination_keys_as_tuples if len(t) > 1]\n",
    "    if (size_of_V,) in multiple_key_combinations:\n",
    "        multiple_key_combinations.remove((size_of_V,))  # Remove from multiple-key list\n",
    "        single_key_combinations.append((size_of_V,))  # Add to single-key list\n",
    "\n",
    "    return single_key_combinations, multiple_key_combinations, combination_keys_as_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e6cdc8-0c69-4f3d-8988-ee976ffc3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prices(accuracies, pricing_methoK, price_per_image, correlation, catalogue, n, images_per_seller, len(indexes)):\n",
    "    keys_K, keys_U, keys = generate_keys(catalogue, n)\n",
    "    prices = pricing_function(accuracies[:n], pricing_methoK, price_per_image, correlation, images_per_seller, len(indexes))\n",
    "    combination_prices_list = []\n",
    "\n",
    "    # Loop over the multiple-key tuples to calculate and store their prices in the list\n",
    "    for combo in keys_U:\n",
    "        # Sum the prices of the components of the combination\n",
    "        combo_price = sum(prices[key - 1] for key in combo)  # Adjust index for 0-based indexing\n",
    "        combination_prices_list.append(combo_price)\n",
    "    \n",
    "    full_prices = np.hstack((np.array(prices), np.array(combination_prices_list)))\n",
    "    \n",
    "    return full_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23374866-b8e2-49b7-b9db-a78db764b97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pricing_function(qualities, pricing_methoK, price_per_image, correlation,images_per_seller):\n",
    "    if pricing_method == \"random\":\n",
    "        return np.random.rand(len(qualities))\n",
    "    \n",
    "    if pricing_method == \"volume\":\n",
    "        return images_per_seller[:size_of_V] * price_per_image\n",
    "    \n",
    "    if pricing_method == \"correlated\":\n",
    "        x1 = np.random.rand(len(qualities))\n",
    "        return  (correlation * qualities + np.sqrt(1-correlation**2)*x1)\n",
    "    \n",
    "    if pricing_method == \"market_based\":\n",
    "        prices = []\n",
    "        for i in range(size_of_V):\n",
    "            if images_per_seller[i]<np.mean(images_per_seller[:size_of_V]) and qualities[i] >= np.mean(qualities[:size_of_V]):\n",
    "                prices.append(images_per_seller[i] * np.random.lognormal(0.617452372, 0.657475287))\n",
    "            else: \n",
    "                prices.append(images_per_seller[i] * np.random.lognormal(-0.714563084, 0.342099957))\n",
    "    \n",
    "        return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788fe9c9-51c2-4788-891c-5db22f7b7597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_U_and_K_sets(original_array, size_of_V, size_of_K, keys):\n",
    "    num_cols = original_array.shape[1]\n",
    "    assert size_of_K <= size_of_V \"size_of_K should not be larger than size_of_V\"\n",
    "    \n",
    "    if size_of_K == size_of_V:\n",
    "        # If N is 10, D includes all first 10 columns, and U includes the rest\n",
    "        K = original_array[:, :size_of_V]\n",
    "        U = original_array[:, size_of_V:]\n",
    "        keys_K = keys[:size_of_V]\n",
    "        keys_U = keys[size_of_V:]\n",
    "    else:\n",
    "        # Randomly selecting N unique column indices from the first size_of_V columns\n",
    "        random_col_indices = np.random.choice(size_of_V, size=size_of_K, replace=False)\n",
    "        # Creating D with the randomly selected columns\n",
    "        K = original_array[:, random_col_indices]\n",
    "        # Creating keys_d with the randomly selected keys\n",
    "        keys_K = [keys[i] for i in random_col_indices]\n",
    "        \n",
    "        # Creating a mask for all columns not included in K\n",
    "        mask = np.ones(num_cols, dtype=bool)\n",
    "        # Set all selected columns in the first size_of_V to False in the mask\n",
    "        mask[random_col_indices] = False\n",
    "        # Ensure that the rest of the first size_of_V columns are correctly identified\n",
    "        for index in range(size_of_V):\n",
    "            if index not in random_col_indices:\n",
    "                mask[index] = True\n",
    "        # Creating U with the remaining columns\n",
    "        U = original_array[:, mask]\n",
    "        # Creating keys_u with the remaining keys\n",
    "        keys_U = [keys[i] for i in range(num_cols) if mask[i]]\n",
    "\n",
    "    return K, U, keys_K, keys_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6422a11d-cfe3-48d5-a2d3-6f0146ccddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_sets(K, U, budget_scale, keys_K, keys_U):\n",
    "    # Calculate the budget ceiling based on the maximum of the second row in U\n",
    "    B = U[1,:]*budget_scale\n",
    "    \n",
    "    # Identify columns in U where the second row does not exceed B\n",
    "    valid_indices_u = np.where(U[1] <= B)[0]\n",
    "   \n",
    "    # Filter U and keys_u accordingly\n",
    "    U = U[:, valid_indices_u]\n",
    "    keys_U = [keys_U[i] for i in valid_indices_u]  # Ensure this list comprehension does not go out of range\n",
    "\n",
    "    # Sort D by the first row in descending order and update keys_d\n",
    "    sorted_indices_k = np.argsort(-K[0])\n",
    "    K = K[:, sorted_indices_k]\n",
    "    keys_K = [keys_K[i] for i in sorted_indices_k]\n",
    "\n",
    "    # Sort U by the second row in increasing order and update keys_u\n",
    "    sorted_indices_u = np.argsort(U[1])\n",
    "    U = U[:, sorted_indices_u]\n",
    "    keys_U = [keys_U[i] for i in sorted_indices_u]\n",
    "    return K, U, keys_K, keys_U, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7b620-a087-4c4c-8af0-eae87aef91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_combinations(data, keys, target_key, exploration_depth, max_price):\n",
    "    # Calculate the size to which sub-combinations must be generated\n",
    "    min_size = len(target_key) - exploration_depth\n",
    "    if min_size < 1:\n",
    "        min_size = 1  # Ensuring that we don't go below size 1 which doesn't make sense\n",
    "\n",
    "    # Generate all sub-combinations from the target key down to min_size\n",
    "    sub_combinations = set()\n",
    "    for size in range(len(target_key), min_size - 1, -1):\n",
    "        for combo in itertools.combinations(target_key, size):\n",
    "            sub_combinations.add(combo)\n",
    "\n",
    "    # Convert list of keys into a set of tuples for faster checking\n",
    "    keys_set = set(map(tuple, keys))\n",
    "\n",
    "    # Find indices to keep (those not in sub_combinations and below max_price)\n",
    "    keep_indices = []\n",
    "    for index, key in enumerate(keys):\n",
    "        if tuple(key) not in sub_combinations and data[1, index] <= max_price:\n",
    "            keep_indices.append(index)\n",
    "\n",
    "    # Filter the data array to keep only the selected indices\n",
    "    new_data = data[:, keep_indices]\n",
    "\n",
    "    # Filter the keys list similarly\n",
    "    new_keys = [keys[i] for i in keep_indices]\n",
    "\n",
    "    return new_data, new_keys       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30645f85-391b-4973-a325-1aa788992a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_task(B, R, N, K, U, risk, method, keys, keys_K, keys_U, gamma, runs, epsilon):\n",
    "    \n",
    "    probabilistic_pricebased_step = np.zeros((runs))\n",
    "    vol_bandit_step = np.zeros((runs))\n",
    "    ent_bandit_step = np.zeros((runs))\n",
    "    rep_bandit_step = np.zeros((runs))\n",
    "    blind_step = np.zeros((runs))\n",
    "    epsilon_step = np.zeros((runs))\n",
    "    \n",
    "    probabilistic_pricebased_count = np.zeros((runs))\n",
    "    vol_count = np.zeros((runs))\n",
    "    ent_count = np.zeros((runs))\n",
    "    rep_count = np.zeros((runs))\n",
    "    blind_count = np.zeros((runs))\n",
    "    epsilon_count = np.zeros((runs))\n",
    "    \n",
    "    complete = np.hstack((K,U))\n",
    "    valid_indices = complete[0][complete[1]<B]\n",
    "    offline = valid_indices.max()\n",
    "    \n",
    "    size_of_U = U.shape[1]  \n",
    "    size_of_K = K.shape[1]\n",
    "    \n",
    "    greedy_performance, greedy_count = greedy(K,U,B,R)\n",
    "    inv_greedy_performance, inv_greedy_count = inv_greedy(K,U,B,R)\n",
    "    \n",
    "    buyer_insight1_performance, buyer_insight1_count, X, price, best_key = knapsack_y(K, U, B, R, keys, keys_K, keys_U, gamma)\n",
    "\n",
    "    U_left, keys_u_left = filter_combinations(U, keys_U, best_key, X, (B - (price + R * buyer_insight1_count)))\n",
    "                                              \n",
    "    while U_left.shape[1] > 0:                                         \n",
    "        buyer_insight2_performance, buyer_insight2_count, X, new_price, best_key = knapsack_y(K, U_left, (B - (price + R * buyer_insight1_count)), R, keys, keys_K, keys_u_left, gamma)\n",
    "            \n",
    "        if buyer_insight2_count == None:\n",
    "            break\n",
    "        else:        \n",
    "            buyer_insight1_count += buyer_insight2_count\n",
    "                                              \n",
    "        if buyer_insight2_performance > buyer_insight1_performance:\n",
    "            buyer_insight1_performance =buyer_insight2_performance\n",
    "            price = new_price\n",
    "            \n",
    "        U_left, keys_u_left = filter_combinations(U_left, keys_u_left, best_key, X, (B - (price + R * buyer_insight1_count)))\n",
    "\n",
    "        \n",
    "    for k in range(runs):\n",
    "        probabilistic_pricebased_step[k], probabilistic_pricebased_count[k] = probabilistic_pricebased(K,U,B,R,risk)\n",
    "        blind_step[k], blind_count[k] = blind_buyer(K,U,B,R)\n",
    "        epsilon_step[k], epsilon_count[k] = epsilon_greedy_bandit(K,U,B,R,epsilon)\n",
    "            \n",
    "    probabilistic_pricebased_performance = np.mean(probabilistic_pricebased_step)\n",
    "    blind_performance = np.mean(blind_step)\n",
    "    epsilon_performance = np.mean(epsilon_step)  \n",
    "    \n",
    "    return offline, greedy_performance, inv_greedy_performance, probabilistic_pricebased_performance, blind_performance, epsilon_performance, buyer_insight1_performance, \n",
    "        greedy_count, inv_greedy_count, buyer_insight1_count, np.mean(probabilistic_pricebased_count), np.mean(blind_count), np.mean(epsilon_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ab2a5-d2c4-4136-a0bf-6f56b78d2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the parameters of the test run\n",
    "\n",
    "correlation = 1 # Set the price-quality correlation value for the test if using correlated pricing\n",
    "iterations = 100 #Set how many iterations you want to run and averag for your results\n",
    "R_values = np.linspace(0.001, 0.2, 40) # Set the range of revelation values you want to test (as % of B)\n",
    "size_of_V = 10 # Set the number of available individual datasets in V\n",
    "size_of_K = 3 # Set the number of datasets revealed for free in the known set K\n",
    "catalogue_size = 1023 # size of the catalog\n",
    "price_per_image = 1 # unit price for the datasets\n",
    "method = \"random\" # Pricing method\n",
    "\n",
    "with open(f\"MNIST_FM.json\", 'r') as f:\n",
    "    accuracies = np.array(json.load(f))\n",
    "with open(f\"MNIST_len.json\", 'r') as f:\n",
    "    images_per_seller = np.array(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cfe109-42f9-4e4f-bf53-c740f9599495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare the empty result arrays\n",
    "offline = np.zeros((len(R_values), iterations))\n",
    "probabilistic_pricebased_performance = np.zeros((len(R_values), iterations))\n",
    "greedy_performance = np.zeros((len(R_values), iterations))\n",
    "inv_greedy_performance = np.zeros((len(R_values), iterations))\n",
    "blind_performance = np.zeros((len(R_values), iterations))\n",
    "epsilon_performance = np.zeros((len(R_values), iterations))\n",
    "buyer_insight_performance = np.zeros((len(R_values), iterations))\n",
    "    \n",
    "#Declare the empty exploration count arrays\n",
    "probabilistic_pricebased_counts = np.zeros((len(R_values), iterations))\n",
    "greedy_counts = np.zeros((len(R_values), iterations))\n",
    "inv_greedy_counts = np.zeros((len(R_values), iterations))\n",
    "blind_counts = np.zeros((len(R_values), iterations))\n",
    "epsilon_counts = np.zeros((len(R_values), iterations))\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "            futures = {}\n",
    "            for y in range(iterations):\n",
    "                full_prices = generate_prices(accuracies[0,:], method, price_per_image, correlation, len(accuracies[0,:]), size_of_V, images_per_seller)\n",
    "                original = np.vstack((accuracies[0,:],full_prices))\n",
    "                K, U, keys_K, keys_U = create_U_and_K_sets(original, size_of_K, keys)\n",
    "                K, U, keys_K, keys_U, B = scale_sets(K, U, budget_scale, keys_K, keys_U)\n",
    "                for x, R in enumerate(R_values):\n",
    "                    R = R * B\n",
    "                    future = executor.submit(process_task, B, R, N, K, U, risk, method, keys, keys_K, keys_U, gamma, runs, epsilon)\n",
    "                    futures[future] = (x,y)\n",
    "                \n",
    "                \n",
    "            for future in as_completed(futures):\n",
    "                x, y = futures[future]\n",
    "                result = future.result()\n",
    "                offline[x,y] = result[0]\n",
    "                greedy_performance[x,y] = result[1]\n",
    "                inv_greedy_performance[x,y] = result[2]\n",
    "                probabilistic_pricebased_performance[x,y] = result[3]\n",
    "                blind_performance[x,y] = result[4]\n",
    "                epsilon_performance[x,y] = result[5]\n",
    "                buyer_insight1_performance[x,y] = result[6]\n",
    "\n",
    "                \n",
    "                greedy_counts[x,y] = result[7]\n",
    "                inv_greedy_counts[x,y] = result[8]\n",
    "                buyer_insight1_counts[x,y] = result[9]\n",
    "                probabilistic_pricebased_counts[x,y] = result[10]\n",
    "                blind_counts[x,y] = result[11]\n",
    "                epsilon_counts[x,y] = result[12]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
