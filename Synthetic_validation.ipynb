{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab01ca-b929-41be-8863-9baa3a11b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from algorithm_greedy import*\n",
    "from algorithm_baselines import*\n",
    "from algorithm_pricebased import*\n",
    "from algorithm_buyer_insight import*\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a789ba-56f2-49c8-b9e3-e47989dae24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the resulting accuracy of a combination given its individual accuracies and the beta parameter\n",
    "def generate_combination_accuracies(accuracies, beta):\n",
    "    accuracies = np.array(accuracies)\n",
    "    scaled_accuracies = accuracies * np.exp(beta - 1)\n",
    "    divisor = len(accuracies) ** (max(0, 1 - beta))  # Increasing divisor for 0 < beta < 1\n",
    "    if beta == 0:\n",
    "        return np.exp(np.mean(np.log(scaled_accuracies)))\n",
    "    power_sum = np.power(scaled_accuracies, beta)\n",
    "    combined_accuracy =  min(1, np.power(np.sum(power_sum) / divisor, 1/beta))\n",
    "    return combined_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3089d63-0a0d-438d-979e-3a018bd9f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the catalog\n",
    "def generate_datasets(beta, size_of_V):\n",
    "    individual_accuracies = np.random.normal(0.15, 0.1, size_of_V) # Generate accuracies for individual datasets from a normal distribution\n",
    "    individual_accuracies = np.clip(individual_accuracies, 0, 1)  # Ensure accuracies are between 0 and 1\n",
    "    \n",
    "    # Generate all possible non-empty combinations of datasets\n",
    "    total_datasets = len(individual_accuracies)\n",
    "    combinations = []\n",
    "    for r in range(1, total_datasets + 1):\n",
    "        for combo in itertools.combinations(individual_accuracies, r):\n",
    "            combinations.append(combo)\n",
    "            \n",
    "    combined_accuracies = np.zeros((len(combinations)))\n",
    "    for x, combo in enumerate(combinations):\n",
    "        combined_accuracies[x] = generate_combination_accuracies(combo, beta)\n",
    "    \n",
    "    combined_accuracies = combined_accuracies[size_of_V:]\n",
    "    return individual_accuracies, combined_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bbf269-2f1e-4dc6-ac15-66fed4ee8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of keys for the combinations in the catalog\n",
    "def generate_keys(catalogue_size, size_of_V):\n",
    "    # Generate the original keys (assuming they are simply integers starting from 1 for simplicity)\n",
    "    original_keys = list(range(1, size_of_V + 1))\n",
    "\n",
    "   # Initialize the list to store the keys representing combinations of original datasets\n",
    "    combination_keys_as_tuples = []\n",
    "\n",
    "    # Start by adding the original datasets' keys as single-element tuples\n",
    "    for key in original_keys:\n",
    "        combination_keys_as_tuples.append((key,))\n",
    "\n",
    "    # Generate combinations of increasing size and create keys representing these combinations as tuples\n",
    "    for n in range(2, len(original_keys) + 1):\n",
    "        for combo in itertools.combinations(original_keys, n):\n",
    "            if len(combination_keys_as_tuples) >= catalogue_size:\n",
    "                # Break early if catalogue limit is reached or about to be reached\n",
    "                break\n",
    "            # Create a tuple from the combo, which already contains integers\n",
    "            combo_tuple = tuple(combo)\n",
    "            combination_keys_as_tuples.append(combo_tuple)\n",
    "\n",
    "    # Check if we reached the limit or need to stop\n",
    "    if len(combination_keys_as_tuples) > catalogue_size:\n",
    "        # Trim the list to the catalogue size if it exceeded the limit\n",
    "        combination_keys_as_tuples = combination_keys_as_tuples[:catalogue_size]\n",
    "\n",
    "    single_key_combinations = [t for t in combination_keys_as_tuples if len(t) == 1]\n",
    "    multiple_key_combinations = [t for t in combination_keys_as_tuples if len(t) > 1]\n",
    "    if (size_of_V,) in multiple_key_combinations:\n",
    "        multiple_key_combinations.remove((size_of_V,))  # Remove from multiple-key list\n",
    "        single_key_combinations.append((size_of_V,))  # Add to single-key list\n",
    "\n",
    "    return single_key_combinations, multiple_key_combinations, combination_keys_as_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff5c0c-e70c-4aa4-9e7c-0b12da1ab0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the prices of the combinations in the catalog\n",
    "def generate_prices(individual_accuracies, combination_accuracies, price_correlation):\n",
    "    all_accuracies = np.hstack((individual_accuracies, combination_accuracies))\n",
    "    keys_K, keys_U, keys = generate_keys((2**len(individual_accuracies))-1, len(individual_accuracies))\n",
    "    \n",
    "    random_list = np.random.normal(size=len(individual_accuracies))\n",
    "    original_standardized = (individual_accuracies - np.mean(individual_accuracies)) / np.std(individual_accuracies)\n",
    "    random_standardized = (random_list - np.mean(random_list)) / np.std(random_list)\n",
    "    new_list = price_correlation * original_standardized + np.sqrt(1 - price_correlation**2) * random_standardized\n",
    "    new_list_scaled = new_list * np.std(individual_accuracies) + np.mean(individual_accuracies)\n",
    "    individual_prices = new_list_scaled / np.max(new_list_scaled)\n",
    "    \n",
    "    combination_prices_list = []\n",
    "    for combo in keys_u:\n",
    "        # Sum the prices of the components of the combination\n",
    "        combo_price = sum(individual_prices[key - 1] for key in combo)  # Adjust index for 0-based indexing\n",
    "        combination_prices_list.append(combo_price)\n",
    "    \n",
    "    full_prices = np.hstack((individual_prices, combination_prices_list))\n",
    "    return full_prices, keys_K, keys_U, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37937520-aed0-477e-9f0a-a4db7fd27b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the set of known K datasets and unknown U combinations\n",
    "def create_U_and_K_sets(original_array, size_of_K, size_of_V, keys):\n",
    "    num_cols = original_array.shape[1]\n",
    "    assert size_of_K <= size_of_V, \"N should not be larger than 10\"\n",
    "    \n",
    "    if N == size_of_V:\n",
    "        # If size_of_K is size_of_V, K includes all first size_of_V columns, and U includes the rest\n",
    "        D = original_array[:, :size_of_V]\n",
    "        U = original_array[:, size_of_V:]\n",
    "        keys_d = keys[:size_of_V]\n",
    "        keys_u = keys[size_of_V:]\n",
    "    else:\n",
    "        # Randomly selecting size_of_K unique column indices from the first size_of_V columns\n",
    "        random_col_indices = np.random.choice(size_of_V, size=size_of_K, replace=False)\n",
    "        # Creating K with the randomly selected columns\n",
    "        K = original_array[:, random_col_indices]\n",
    "        # Creating keys_d with the randomly selected keys\n",
    "        keys_K = [keys[i] for i in random_col_indices]\n",
    "        \n",
    "        # Creating a mask for all columns not included in K\n",
    "        mask = np.ones(num_cols, dtype=bool)\n",
    "        # Set all selected columns in the first size_of_V to False in the mask\n",
    "        mask[random_col_indices] = False\n",
    "        # Ensure that the rest of the first size_of_V columns are correctly identified\n",
    "        for index in range(size_of_V):\n",
    "            if index not in random_col_indices:\n",
    "                mask[index] = True\n",
    "        # Creating U with the remaining columns\n",
    "        U = original_array[:, mask]\n",
    "        # Creating keys_U with the remaining keys\n",
    "        keys_U = [keys[i] for i in range(num_cols) if mask[i]]\n",
    "\n",
    "    return K, U, keys_K, keys_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5767d757-c315-44a5-92f7-f547a16da8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the sets into K(B) and U(B)\n",
    "def scale_sets(K, U, budget_scale, keys_K, keys_U):\n",
    "    # Calculate the budget ceiling based on the maximum of the second row in U\n",
    "    B = U[1,:]*budget_scale\n",
    "    \n",
    "    # Identify columns in U where the second row does not exceed B\n",
    "    valid_indices_u = np.where(U[1] <= B)[0]\n",
    "   \n",
    "    # Filter U and keys_u accordingly\n",
    "    U = U[:, valid_indices_u]\n",
    "    keys_U = [keys_U[i] for i in valid_indices_u]  # Ensure this list comprehension does not go out of range\n",
    "\n",
    "    # Sort K by the first row in descending order and update keys_K\n",
    "    sorted_indices_k = np.argsort(-K[0])\n",
    "    K = K[:, sorted_indices_k]\n",
    "    keys_d = [keys_d[i] for i in sorted_indices_d]\n",
    "\n",
    "    # Sort U by the second row in increasing order and update keys_u\n",
    "    sorted_indices_u = np.argsort(U[1])\n",
    "    U = U[:, sorted_indices_u]\n",
    "    keys_U = [keys_U[i] for i in sorted_indices_u]\n",
    "    return K, U, keys_K, keys_U, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c63f5-8775-4a92-9bd4-fa528866b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the algorithms and output their performance and exploration counts\n",
    "def process_task(B, R, K, U, keys, keys_K, keys_U, risk, gamma, epsilon, runs):\n",
    "    probabilistic_pricebased_step = np.zeros((runs))\n",
    "    blind_step = np.zeros((runs))\n",
    "    epsilon_step = np.zeros((runs))\n",
    "    \n",
    "    probabilistic_pricebased_count = np.zeros((runs))\n",
    "    blind_count = np.zeros((runs))\n",
    "    epsilon_count = np.zeros((runs))\n",
    "    \n",
    "    complete = np.hstack((K,U))\n",
    "    valid_indices = complete[0][complete[1]<B]\n",
    "    offline = valid_indices.max()\n",
    "    \n",
    "    size_of_U = U.shape[1]  \n",
    "    size_of_D = D.shape[1]\n",
    "    \n",
    "    greedy_performance, greedy_count = greedy(K,U,B,R)\n",
    "    inv_greedy_performance, inv_greedy_count = inv_greedy(K,U,B,R)\n",
    "    buyer_insight_performance, buyer_insight_count = buyer_insight_y(K, U, B, R, keys, keys_K, keys_U, gamma)\n",
    "\n",
    "    for k in range(runs):\n",
    "        probabilistic_pricebased_step[k], cor_count[k] = probabilistic_pricebased(K,U,B,R,risk)\n",
    "        blind_step[k], blind_count[k] = blind_buyer(K,U,B,R)\n",
    "        epsilon_step[k], epsilon_count[k] = epsilon_greedy_bandit(K,U,B,R,epsilon)\n",
    "            \n",
    "    probabilistic_pricebased_performance = np.mean(probabilistic_pricebased_step)\n",
    "    blind_performance = np.mean(blind_step)\n",
    "    epsilon_performance = np.mean(epsilon_step)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    return offline, greedy_performance, inv_greedy_performance, probabilistic_pricebased_performance, blind_performance, epsilon_performance, buyer_insight_performance, \n",
    "            greedy_count, inv_greedy_count, np.mean(probabilistic_pricebased_count), np.mean(blind_count), np.mean(epsilon_count), buyer_insight_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7504ab-aa1e-4117-a22d-8eb7d61f384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the parameters of the test run\n",
    "\n",
    "beta = 1 # Set the beta value for the test run\n",
    "price_cor = 0 # Set the price-quality correlation value for the test\n",
    "iterations = 1 #Set how many iterations you want to run and averag for your results\n",
    "R_values = np.linspace(0.001, 0.2, 40) # Set the range of revelation values you want to test (as % of B)\n",
    "size_of_V = 10 # Set the number of available individual datasets in V\n",
    "size_of_K = 3 # Set the number of datasets revealed for free in the known set K\n",
    "risk = 0.2 # Set the sacrifice hyperparameter for the probabilistic price-based algorithm\n",
    "gamma = 1 # Set the gamma hyperparameter for the deterministic buyer-insight algorithm\n",
    "runs = 100 # Set the number of runs to run the probabilistic algorithms to average the performance\n",
    "epsilon = 0.9 #Set the probability of epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0cfd91-4e5e-4be5-8960-729896c158e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare the empty result arrays\n",
    "offline = np.zeros((len(R_values), iterations))\n",
    "probabilistic_pricebased_performance = np.zeros((len(R_values), iterations))\n",
    "greedy_performance = np.zeros((len(R_values), iterations))\n",
    "inv_greedy_performance = np.zeros((len(R_values), iterations))\n",
    "blind_performance = np.zeros((len(R_values), iterations))\n",
    "epsilon_performance = np.zeros((len(R_values), iterations))\n",
    "buyer_insight_performance = np.zeros((len(R_values), iterations))\n",
    "    \n",
    "#Declare the empty exploration count arrays\n",
    "probabilistic_pricebased_counts = np.zeros((len(R_values), iterations))\n",
    "greedy_counts = np.zeros((len(R_values), iterations))\n",
    "inv_greedy_counts = np.zeros((len(R_values), iterations))\n",
    "blind_counts = np.zeros((len(R_values), iterations))\n",
    "epsilon_counts = np.zeros((len(R_values), iterations))\n",
    "buyer_insight_counts = np.zeros((len(R_values), iterations))\n",
    "\n",
    "\n",
    "#Execute the functions and run the test-bench\n",
    "individual_accs, combination_accs = generate_datasets(beta)\n",
    "full_accuracies = np.hstack((individual_accs, combination_accs))\n",
    "\n",
    "full_prices, keys_K, keys_u, keys = generate_prices(individual_accs, combination_accs, price_cor)\n",
    "original = np.vstack((full_accuracies, full_prices))\n",
    "        \n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = {}\n",
    "        for y in range(iterations):\n",
    "            K, U, keys_K, keys_U = create_U_and_K_sets(original, size_of_K, size_of_V, keys)\n",
    "            K, U, keys_K, keys_U, B = scale_sets(K,U, budget_scale, keys_K, keys_U)\n",
    "                for x, R in enumerate(R_values):\n",
    "                    R = R * B\n",
    "                    future = executor.submit(process_task, B, R, K, U, keys, keys_K, keys_U, risk, gamma, epsilon, runs)\n",
    "                    futures[future] = (x,y)\n",
    "                \n",
    "                \n",
    "            for future in as_completed(futures):\n",
    "                x, y = futures[future]\n",
    "                result = future.result()\n",
    "                offline[x,y] = result[0]\n",
    "                greedy_performance[x,y] = result[1]\n",
    "                inv_greedy_performance[x,y] = result[2]\n",
    "                probabilistic_pricebased_performance[x,y] = result[3]\n",
    "                blind_performance[x,y] = result[4]\n",
    "                epsilon_performance[x,y] = result[5]\n",
    "                buyer_insight_performance[x,y] = result[6]\n",
    "\n",
    "\n",
    "                greedy_counts[x,y] = result[7]\n",
    "                inv_greedy_counts[x,y] = result[8]\n",
    "                probabilistic_pricebased_counts[x,y] = result[9]\n",
    "                blind_counts[x,y] = result[10]\n",
    "                epsilon_counts[x,y] = result[11]\n",
    "                buyer_insight_counts[x,y] = result[12]\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
